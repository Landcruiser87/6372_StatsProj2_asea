---
title: "Health Analysis"
author: "asea"
date: "April 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(car)
library(Hmisc)
library(gplots)
library(ggplot2)
library(ROCR)
library(MASS)
library(corrplot)
library(randomForest)
library(epitools)
library(dplyr)
library(glmnet)
```

The national dataset used in this study comes from the 2017 County Health Rankings website. Since each state and county have varying populations, we chose to use the data as a percent of the population. The death rate is the age-adjusted Years of Potential Life Lost rate per 100,000. All variables are continuous so we do not need to be concerned about unbalanced categorical variables.

OBJECTIVE1

We want to model the probability of being obsese (BMI > 30) for a particular variable. The team has decided to use the following variables as the predictors.The variable data represent the percentage of the population for a specifc US county.

Health Variables
- Smokers
- Physically Incactive
- Excesive Drinking
- Frequent Mental Distress
- Frequent Physical Distress
- Diabetic
- Insufficient Sleep

Demographic Type Variables
- Uninsured
- Some College 
- Unemployed
- Severe Housing Problems


EXPLORATORY DATA ANALYSIS

```{r EDA Start}
# import health data
alldata = read.csv("./Data/CHR_Data.csv")

# get initial data
data <- alldata[,-c(1:3,4,14,18:24)]
datarf <- alldata[,-c(1:4)]
# show data summary to identify variables with missing data
summary(data)
```

The summary statistics show missing values for Premature_Death_Rate, Unemployed, Uninsured, and Graduation_Rate. We impute the missing data with the median values since some distributions show a little skew when comparing the mean and medians. We will later show the histograms confirm some skew.

```{r EDA Impute}
# impute variables with their medians, prefer medians in the event there is any skew
data$Unemployed <- with(data,impute(Unemployed,median))
data$Uninsured <- with(data,impute(Uninsured,median))
```

EDA-SKEW 

The histograms confirm some skew with Frequent_Mental_Distress, Frequent_Physical_Distress,Uninsured, Unemployed, and Severe_Housing_Problems. There is no concern or need to do anything about the skew since we have a relatively large data set and we are performing a logistic regression. 

```{r EDA Histograms}
# get chosen predictors for EDA
predictors <- data[,-2]

# histogram of chosen predictors
par(mfrow=c(3,4))
for(i in 1:11) {
    hist(predictors[,i], main=names(predictors)[i])
}
```

OUTLIERS (Used SAS, will put screen shots and comments about Cook's D in final paper)

We remove the data for Yuma county in Arizona due to the outlier it creates for unemployment. The county is along the Mexico border and is predominately a farming community with migrant (seasonal) workers. This situation is uncommon and not typical of U.S. counties. We also removed the data for Imperial county in California for the same reasons. It is adjacent to Yuma county.

We remove the data for Bethel, Northwest Arctic and Yukon-Koyukuk counties in Alaska for Severe Housing Problems. There are four factors that contribute to this category. They are housing units that lack complete kitchens, lack complete plumbing facilities, overcrowded, or severely cost burdened. These counties reside in Alaska where the cost to build is beyond what the residents can afford and therefore overcrowding is above normal compared to the rest of the United States. [Nathan Wiltse, Dustin Madden, 2018 Alaska Housing Assessment, Jan 17, 2018, https://www.ahfc.us/download_file/view/5124/853]

```{r Remove Outliers}
# print data before outlier removal
plot(data$Unemployed)
plot(data$Severe_Housing_Problems)

data <- data[!rowSums(data[c(-1:-10,-12)] > 20),] # removed unemployment outliers for migrant farming counties
data <- data[!rowSums(data[c(-1:-11)] > 50),] # removed housing prob outliers for poor Alaska counties

# print data after outlier removal
plot(data$Unemployed)
plot(data$Severe_Housing_Problems)
```

```{r EDA Scatter Plots}
# create obese binary classification where BMI >= 30 is considered Obese, 0 = not obese, 1 = obese
data$Obese_Class <- 1
data[data$Obese < 30, "Obese_Class"] <- 0
data$Obese_Class <- as.factor(as.character(data$Obese_Class))

# -- create new data set with obese_class first --
data2 <- data[,c(13,1,3:12)]


# summary stats by group Obese_Class, to add to SAS Box Plots
t(aggregate(Smokers~Obese_Class,data=data2,summary))
t(aggregate(Physically_Inactive~Obese_Class,data=data2,summary))
t(aggregate(Excessive_Drinking~Obese_Class,data=data2,summary))
t(aggregate(Frequent_Mental_Distress~Obese_Class,data=data2,summary))
t(aggregate(Frequent_Physical_Distress~Obese_Class,data=data2,summary))
t(aggregate(Diabetic~Obese_Class,data=data2,summary))
t(aggregate(Insufficient_Sleep~Obese_Class,data=data2,summary))
t(aggregate(Uninsured~Obese_Class,data=data2,summary))
t(aggregate(Some_College~Obese_Class,data=data2,summary))
t(aggregate(Unemployed~Obese_Class,data=data2,summary))
t(aggregate(Severe_Housing_Problems~Obese_Class,data=data2,summary))

# scatter matrices, with Obese as the colors
pairs(data2[,2:12],col=data$Obese_Class)
```

For the most part, the colored scatter plot matrix tells us our variables should do a decent job with logistic regression based on the color separation seen in the matrix. Strong correlation  is seen between the following:
<ul>
<li>Frequenet_Mental_Distress, Physical_Mental_Distress</li>
</ul>
There's fairly good correlation between the following:
<ul>
<li>Smokers, Frequent_Mental_Distress, Physical_Mental_Distress</li>
<li>Diabetic, Physically_Inactive, Insufficient Sleep, Frequent_Mental_Distress, Frequent_Physical_Distress.</li>
</ul>

We'll review a correlation heatmap to get better insights next.

```{r EDA Heatmap}
# predictor heatmap correlations to examine whether variables are redundant
cor1 <- cor(data2[,2:12])

heatmap.2(cor1,col=redgreen(75), cexRow=.7,cexCol=0.7,
          density.info="none", trace="none", dendrogram=c("both"), 
          symm=F,symkey=T,symbreaks=T, scale="none",key=T)
```

The dendogramed heatmap confirms the strong correlation previously seen with Frequenet_Mental_Distress and Physical_Mental_Distress.


Additional correlation is seen between the following:
<ul>
<li>Unemployed, Insufficient Sleep</li>
<li>Some_College, Excessive_Drinking</li>
<li>Diabetic, Physically_Inactive</li>
<li>Smokers, Frequenet_Mental_Distress, Physical_Mental_Distress</li>
<li>Uninured, Severe_Housing_Problems</li>
</ul>

The correlations identified by the dendogram surprisingly all make practical sense. One would expect to lose sleep if they were unemployed. Drinking being correlated to college makes sense. Diabetic is not uncommon amongst physically incative people. If someone is living in an area with severe housing problems, we might expect they would not be able to afford insurance.

Let's review correlation with ratings seen in a variable correlation heatmap.

```{r EDA CorrPlot}
#Correlation Plot 
cor2 <- cor(data2[,2:12])
df_corr <-corrplot(cor2, type="upper", addCoef.col = "white", number.digits = 2, number.cex = 0.5, method="square", order="hclust", title="Variable Corr Heatmap",tl.srt=45, tl.cex = 0.8)
```

Based on the variable correlation heatmap, the order of correalated variables are:
<ol>
<li>Frequenet_Mental_Distress, Physical_Mental_Distress</li>
<li>Smokers, Frequenet_Physical_Distress</li>
<li>Smokers, Frequenet_Mental_Distress</li>
<li>Diabetic, Physically_Inactive</li>
<li>Frequent_Mental_Distress, Insufficient Sleep</li>
<li>Unemployed, Frequent_Mental_Distress</li>
<li>Unemployed, Frequent_Physical_Distress</li>
<li>Excessive_Drinking, Frequent_Physical_Distress</li>
<li>Excessive_Drinking, Frequent_Mental_Distress</li>
<li>Diabetic, Frequent_Mental_Distress</li>
</ol>

Let's look at the VIFs to confirm the collinear variables.

```{r VIF Check}
# Logistics Regression
glm.vifchk <- glm(Obese_Class ~ ., data = data2, family = binomial(link="logit"))
vif(glm.vifchk)
```

The VIFS and multiple visual tools agree there is a strong relationship between Frequent_Mental_Distress and Frequent_Physical_Distress. We choose to remove Frequent_Physical_Distress. 

```{r Remove Highly Correlated Variable}
# Remove Frequent_Physical_Distress from data2
data3 <- data2[,-6] 
```

Lets use PCA to visualize any other insights. It is fortunate to already have our data somewhat normalized on a percentage scale. It reduces the scale sensitivity seen with PCA. 

```{r PCA}
dat.x <- data3[,2:11]
dat.y <- data3[,1]
pc.result<-prcomp(dat.x,scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$Obese_Class<-dat.y

#Loadings for interpretation
pc.result$rotation
```

```{r Scree Plot}
# Scree plot
pc.eigen<-(pc.result$sdev)^2
pc.prop<-pc.eigen/sum(pc.eigen)
pc.cumprop<-cumsum(pc.prop)
plot(1:10,pc.prop,type="l",main="Scree Plot",ylim=c(0,1),xlab="PC #",ylab="Proportion of Variation")
axis(1, seq(1,10,1))
lines(1:10,pc.cumprop,lty=3)

# Cumulative proportion plot
cumulative.prop<-cumsum(pc.eigen/sum(pc.eigen))
plot(1:10,cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0.5,1))
points(x=6, y=0.9, type="p", pch=10, col="green")
```

The cumulative plot shows 6 PCs are needed to retain ~90% of the total variation in the data.

```{r Plot some Principal Components}
#Use ggplot2 to plot pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC3)) +
  geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
  geom_vline(xintercept = 0, colour = "gray65") +
  ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC4)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC5)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC6)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC4)) +
  geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
  geom_vline(xintercept = 0, colour = "gray65") +
  ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC5)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC6)) +
    geom_point(aes(col=Obese_Class), size=1)+
  geom_hline(yintercept = 0, colour = "gray65") +
    geom_vline(xintercept = 0, colour = "gray65") +
    ggtitle("PCA plot of Health Data")

```

The PCA plots show some separation but not great separation. We should still be able to get decent results using multiple variable but we will use the original variables for our model.

For our model, we choose a logistic regression using LASSO for feature reduction.

```{r First Pass Prob1 Log Regress}
glm.fit <- glm(Obese_Class ~ ., data = data3, family = binomial)
summary(glm.fit)
```

After different assessments and iterations, we agree with removing the following suggested insignificant predictors:
- Excessive_Drinking
- Insufficient_Sleep
- Some_College
- Unemployed
- Sever_Housing_Problems

```{r Remove Insig Vars}
# removed insignifcant predictors
data4 <- data3[,-c(4,7,9:11)]
```

Rerun the model with signifcant predictor.

```{r Final Prob1 Log Regress}
glm.fit <- glm(Obese_Class ~ ., data = data4, family = binomial)
summary(glm.fit)
confint(glm.fit)

# build a training & test data set
samplesize=nrow(data4)
train_percent = .8
train_indices = sample(seq(1,samplesize,length = samplesize),train_percent*samplesize) # get random indices
train = data4[train_indices,] # random training data
test = data4[-train_indices,] # random test data

train.x <-train[,2:ncol(train)]
train.y <-train[,1]


#--glm ROC--
glmpred<-predict(glm.fit, newdata = train.x, type = "response")

pred <- prediction(glmpred, train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot glm ROC
plot(roc.perf,main="GLM")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .25, y = .75,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

```

Our final fitted model for Problem 1 is shown below.

<font color="blue">
log(p/(1-p)) = logit(p) = -10.18016 + 0.42933(Smokers) + 0.26783(Physically_Inactive) - 0.37621(Frequent_Mental_Distress) + 0.03314(Insufficient_Sleep)
</font>

Ho: There is no relationship between the predictor variables and whether someone is obese or not.
Ha: There is a relationship between the predictor variables and whether someone is obese or not.

We reject the null hypothesis with multiple predictors having p-values < 0.05.

There is sufficient evidence at teh alpha = 0.05 level of significance to suggest 

Beta0 = -10.1812 with 95% CI (-11.1835,-9.2105)
The odds of being obese with all variables =t 0 is exp(-10.1812)=0.000038.

Beta1 = 0.42933 with a 95% CI (0.37007,0.49044)
This fitted model says, holding all other variables constant, the odds of being obese for smokers is exp(0.42933)=1.5362 over the odds of not being obese. In terms of percent change, the odds for being obese are 53.62% higher than the odds for not being obese.

Beta2 = 0.26783 with a 95% CI (0.23894, 0.29762)
The coefficient for Physically_Inactive, holding all other variables constant, says the odds of being obese is exp(0.26783)=1.3071 over the odds of not being obese. In terms of percent change, the odds for being obese are 30.71% higher than the odds for not being obese. 

Beta3 = -0.37621 with a 95% CI (0.-45264, -0.302)
The coefficient for Physically_Inactive, holding all other variables constant, says the odds of being obese is exp(-0.37621)=0.6865 over the odds of not being obese. In terms of percent change, the odds for being obese are 31.35% lower than the odds for not being obese. 

Beta4 = 0.03314 with a 95% CI (0.00246, 0.06386)
The coefficient for Physically_Inactive, holding all other variables constant, says the odds of being obese is exp(0.03314)=1.0337 over the odds of not being obese. In terms of percent change, the odds for being obese are 3.37% higher than the odds for not being obese. 

Goodness of Fit: No categorical variables in our data so there is no need to do a Chi Square test.



```{r Extra Logistic Reg}

# Logistics Regression
glm.fit <- glm(Obese_Class ~ ., data = data4, family = binomial)
summary(glm.fit)

#Confidence Intervals
confint.default(glm.fit) #Wald
confint(glm.fit) #Profile

#odds ratio
exp(cbind(odd = coef(glm.fit), confint(glm.fit)))

#checking group proportions
#proportion <- rbind(table(train$Obese_Class),table(test$Obese_Class))
#dimnames(proportion)<- list("Data Set"=c("Train","Test"), "Obese_Class"=c("Not obese","Obese"))
#proportion


#Model prediction/Accuracy
#glm.pred <- glm.fit %>% predict(train, type = "response")
#predicted.classes <-ifelse(glm.pred < 0.3, 0, 1)
#observed.classes <- test$Obese_Class
#mean(predicted.classes == observed.classes)
```


MODEL CHECK



OPEN ACTIONS: 

TEAM TO REVIEW ANALYSiS (Any other goodness of fit test to do?, Add comments througout incl conclusion)



========================================================================================

OBJECTIVE 2:

With a simple logistic regression model as a baseline, perform additional competing models to improve on prediction performance metrics.

OPEN ACTIONS:
1. Do a train/test split on the Prob1 model and get Accuracy/Specificity (data set = data4). Now we have a baseline to compare.
2. Add an interactive term (alcohol x smoking, mental stress x unemployed ) and rerun the model to compare to the baseline and comment.
3. Run the KNN model to compare
4. Run Random Forrest to compare


```{r Split Data Into Training and Test }
# build a training & test data set
samplesize=nrow(data4)
train_percent = .8
train_indices = sample(seq(1,samplesize,length = samplesize),train_percent*samplesize) # get random indices
train = data3[train_indices,] # random training data
test = data3[-train_indices,] # random test data

# check train & test for training bias, shoudl see similar proportions of 0s & 1s in each
summary(train$Obese_Class)
summary(test$Obese_Class)

train.x <- train[,2:ncol(train)]
train.y <- train[,1] # Obese_Class is in column 1
```


```{r Log Regression Model for Prediction}
train.x <- as.matrix(train.x) # glmnet requires a matrix 
cvfit <- cv.glmnet(train.x, train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")

# predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = train.x, type = "response")

#Compare the prediction to the real outcome
head(fit.pred)
head(train.y)
```

NOT SURE WE NEED THIS
Get Variance Stats
```{r Variance}
round(cov(data4[2:6]), 2) 
sum(diag(cov(data4[2:6]))) # total variance
```

```{r Steve LDA}
samplesize=nrow(data4)
train_percent = .75
train_indices = sample(seq(1,samplesize,length = samplesize),train_percent*samplesize) # get random indices
train = data3[train_indices,] # random training data 
test = data3[-train_indices,] # random test data 

lda <- lda(Obese_Class~.,data = train)
qda <- qda(Obese_Class~.,data = train)

#lda confusion matrix
lda_prd<-predict(lda, newdata = test)$class
lda_cm<-table(lda_prd,test$Obese_Class)
lda_cm

#qda confusion matrix
qda_prd<-predict(qda, newdata = test)$class
qda_cm<-table(qda_prd,test$Obese_Class)
qda_cm

#lda overall misclassification rrror rate
lda_ME<-(lda_cm[2,1]+lda_cm[1,2])/(lda_cm[1,2]+lda_cm[2,2])
lda_ME

#lda Overall Accuracy
1-lda_ME

#qda overall misclassification rrror rate
qda_ME<-(qda_cm[2,1]+qda_cm[1,2])/(qda_cm[1,2]+qda_cm[2,2])
qda_ME

#qda Overall Accuracy
1-qda_ME

#--lda ROC--
ldaprd<-predict(lda, newdata = train)$posterior
#correcting for the way lda creates predicted probabilities
ldaprd<-ldaprd[,2]

pred <- prediction(ldaprd, train$Obese_Class)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot lda ROC
plot(roc.perf,main="LDA")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .25, y = .75,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

#--qda ROC--
qdaprd<-predict(qda, newdata = train)$posterior
#correcting for the way lda creates predicted probabilities
qdaprd<-qdaprd[,2]

pred <- prediction(qdaprd, train$Obese_Class)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot lda ROC
plot(roc.perf,main="QDA")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .25, y = .75,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

```{r Rando_Forrest}
# #have to reimport and reimpute some data.  Annoying, apologies. 
# datarf$Unemployed <- with(datarf,impute(Unemployed,median))
# datarf$Uninsured <- with(datarf,impute(Uninsured,median))
# datarf <- datarf[!rowSums(datarf[c(-1:-10,-13)] > 20),] 
# datarf <- datarf[!rowSums(datarf[c(-1:-11)] > 50),] 
# datarf$Obese_Class <- 1
# datarf[datarf$Obese < 30, "Obese_Class"] <- 0
# datarf$Obese_Class <- as.factor(as.character(datarf$Obese_Class))
# 
# datarf <- datarf[,c(25,5,7:18,24)]
# Obsese.full.forrest <- randomForest(Obese_Class~., data=data4, importance = TRUE)
# importance(rf_model) #Variable importance for placement order (Forward, Backward, Stepwise) 
# varImpPlot(Obsese.full.forrest,type=1, main='Random Tree Variable Importance') 

```

```{r trees setup}
library(rgl)
library(tree)
library(ISLR)
library(randomForest)
```

```{r trees}
#reset dataset
data <- read.csv("./Data/CHR_Data.csv")
#remove the id numbers, need to remove the state and county because that doesn't work with trees
data <- data[,-c(1:3)]
attach(data)
```

```{r repeat adjustment of variables}
# impute variables with their medians, prefer medians in the event there is any skew
data$Premature_Death_Rate <- with(data,impute(Premature_Death_Rate,median)) # may use this later so imputing now
data$Unemployed <- with(data,impute(Unemployed,median))
data$Uninsured <- with(data,impute(Uninsured,median))
data$Graduation_Rate <- with(data,impute(Graduation_Rate))

# rescale Premature_Death_Rate so min value = 2.947 vs 2947, just a preference so it's not scaled so diff from predictors
data$Premature_Death_Rate <- data$Premature_Death_Rate/1000
```

```{r deep tree}
mytree <- tree(Premature_Death_Rate~., data)
plot(mytree)
text(mytree,pretty=0)
```

This tree shows that Frequent Physical Distress is a main split point for premature deaths.

```{r}
#cross validation tree
set.seed(3)
cv.adver=cv.tree(mytree,FUN=prune.tree,method="deviance")
names(cv.adver)
cv.adver
plot(cv.adver)
par(mfrow=c(1,1))
plot(cv.adver$size,cv.adver$dev,type="b")
```

```{r}
#compare normal regression, knn, and ols methods using MSE
set.seed(123)
index<-sample(1:3136,1568)
train<-data[index,]
test<-data[-index,]

train.tree<-tree(Premature_Death_Rate~.,train)
summary(train.tree)
plot(train.tree)
text(train.tree,pretty=0)
plot(cv.tree(train.tree,FUN=prune.tree,method="deviance"))

testMSE<-mean((test$Premature_Death_Rate - predict(train.tree,newdata=test) )^2)


knn<-FNN::knn.reg(train = train[,-c(3,4)], test =test[,-c(3,4)], y = train$Premature_Death_Rate, k = 5)
testMSE.knn<-mean( ( test$Premature_Death_Rate-knn$pred)^2)

full.fit<-lm(Premature_Death_Rate~.,train)
testMSE.ols<-mean((test$Premature_Death_Rate-predict(full.fit,test))^2)


testMSE
testMSE.knn
testMSE.ols
```



The OLS method has the lowest MSE of 1.970053.

```{r knn}
library(class)

data$Premature_Death_Rate <- with(data,impute(Premature_Death_Rate,median)) # may use this later so imputing now
data$Unemployed <- with(data,impute(Unemployed,median))
data$Uninsured <- with(data,impute(Uninsured,median))
data$Graduation_Rate <- with(data,impute(Graduation_Rate))

knn_begin <- data


knn_begin$Unemployed <- as.numeric(knn_begin$Unemployed)
knn_begin$Uninsured <- as.numeric(knn_begin$Uninsured)





knn_lapp <- as.data.frame(lapply(knn_begin[,c(1:12,14,15)],function(x) as.numeric(x))) #Changes all of the columns to type numeric
#knn_lapp <- cbind(knn_lapp,State = knn_begin$State) #combines the new numeric type dataframe with the State column so we have our complete dataframe


knn_lapp <- cbind(knn_lapp, Obese_Class = data$Obese_Class) #This combines the Obese_Class into our dataframe without getting it caught in the lapply()


knn_train <- knn_lapp[1:2500,]  #Creating the Train and test sets
knn_test<- knn_lapp[2501:3136,]

knn_train_target <- as.data.frame(knn_lapp[1:2500,15])

knn_test_target <- as.data.frame(knn_lapp[2501:3136,15])
str(knn_test_target)
str(knn_lapp)

knn_pred_50 <- as.data.frame(knn(train=knn_train,test=knn_test,cl=knn_lapp[1:2500,15],k=50))
#We will start with k at 50, which is approximately the square root of the n,3136.

comparison <- cbind(knn_test_target,Predicted = 
                      knn_pred_50)
colnames(comparison)<- c("TrainTarget","K50.PredictedValue")
head(comparison)


comparison.test <- comparison  #Changing the two variables to numeric allows us to calculate
                               #How accurate our knn model was
comparison.test$TrainTarget <- as.numeric(comparison.test$TrainTarget)
comparison.test$K50.PredictedValue <- as.numeric(comparison.test$K50.PredictedValue)
head(comparison.test)
mean(comparison.test$K50.PredictedValue)

head(comparison.test)

comparison.test$ErrorTerm50 <- abs(comparison.test$TrainTarget-comparison.test$K50.PredictedValue)

100*(1-(mean(comparison.test$ErrorTerm50))) #This value is the percent our model was accurate in predicting the Obesity Class.  When we changed the type from factor to numeric,0 became 1 and 1 became 2.  The k=50 Model was accurate 56.29% of the time

k100.PredictedValue <- as.data.frame(knn(train=knn_train,test=knn_test,cl=knn_lapp[1:2500,15],k=100))
head(k100.PredictedValue)
k100.PredictedValue <- as.numeric(k100.PredictedValue$`knn(train = knn_train, test = knn_test, cl = knn_lapp[1:2500, 15], k = 100)`)

comparison.test <- cbind(comparison.test, k100.PredictedValue = k100.PredictedValue)
comparison.test$ErrorTerm100 <-   (abs(comparison.test$TrainTarget-comparison.test$k100.PredictedValue))/comparison.test$TrainTarget


100*(1-(mean(comparison.test$ErrorTerm100))) #Changing k to 100 improves our accuracy, and we now predicted the class correctly 58.1% of the time.




```